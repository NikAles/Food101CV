{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13c12a1",
   "metadata": {},
   "source": [
    "# Библы и функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6454c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b75c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.cuda import is_available\n",
    "device = \"cuda\" if is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75183d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfrom_first_nn = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transfrom_first_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a78beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_names = ['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n",
    "food_or_not = ['Food', 'Not Food' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33881a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfrom_second_nn = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transfrom_second_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198a91f",
   "metadata": {},
   "source": [
    "# Prediction Of The First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ba455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = torch.load(\"models/first_model_effnet_b0.pth\", weights_only=False).to(device)\n",
    "second_model = torch.load(\"models/second_model_TiV_16_b.pth\", weights_only=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597edec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "#while True:\n",
    " #   ret, frame = cap.read()\n",
    "#    image = transfrom_first_nn(torch.from_numpy(frame))\n",
    " #   transformed_image = transfrom_first_nn(image).unsqueeze(dim=0).to(device)\n",
    " #   first_model.eval()\n",
    " #   if not ret:\n",
    " #       break\n",
    "\n",
    " #   cv2.imshow('Camera', frame)\n",
    "\n",
    "  #  if cv2.waitKey(1) == ord('q'):\n",
    " #       break\n",
    "\n",
    " #   with torch.inference_mode():\n",
    "  #      proba = first_model(transformed_image)\n",
    "   #     logit = torch.round(torch.sigmoid(proba)).type(torch.int16)\n",
    "    #    pred = food_or_not[logit]\n",
    "       # if logit == 0:\n",
    "     #       second_model.eval()\n",
    "      #      with torch.inference_mode():\n",
    "       #         image = transfrom_second_nn(image)\n",
    "            #    second_proba = second_model(transformed_image)\n",
    "###          #plt.axis(False)\n",
    "   #     else:\n",
    "    #        print(\"На фото нет еды\")\n",
    "     #   zxc = torch.topk(torch.round(torch.softmax(second_proba, dim=1), decimals=3)*100, k=3)\n",
    "      #  for i in range(0, 3):\n",
    "       #     print(f\"{classes_names[zxc.indices[0][i]]} - {zxc.values[0][i]:.2f}%\")\n",
    "#cap.release()\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfffb5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     43\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, FPS_text, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m125\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.6\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     45\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     50\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0) \n",
    "t0 = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    transformed_image_first = transfrom_first_nn(pil_image).unsqueeze(dim=0).to(device)\n",
    "\n",
    "    first_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        proba = first_model(transformed_image_first)\n",
    "        logit = torch.round(torch.sigmoid(proba)).type(torch.int16)\n",
    "        pred = food_or_not[logit]\n",
    "\n",
    "\n",
    "        if logit == 0:\n",
    "            second_model.eval()\n",
    "            with torch.inference_mode():\n",
    "\n",
    "                transformed_image_second = transfrom_second_nn(pil_image).unsqueeze(dim=0).to(device)\n",
    "                second_proba = second_model(transformed_image_second)\n",
    "\n",
    "                zxc = torch.topk(torch.round(torch.softmax(second_proba, dim=1), decimals=3)*100, k=3)\n",
    "                topk_first = f\"{classes_names[zxc.indices[0][0]]} - {zxc.values[0][0]:.2f}%\"\n",
    "                topk_second = f\"{classes_names[zxc.indices[0][1]]} - {zxc.values[0][1]:.2f}%\"\n",
    "                topk_third = f\"{classes_names[zxc.indices[0][2]]} - {zxc.values[0][2]:.2f}%\"\n",
    "\n",
    "\n",
    "                text_pred = f\"Type: {pred} Pred: {classes_names[torch.argmax(second_proba)]}\"\n",
    "                cv2.putText(frame, text_pred, (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, topk_first, (5, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, topk_second, (5, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, topk_third, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        FPS = 1/(time.time() - t0)\n",
    "        t0= time.time()\n",
    "        FPS_text = f\"FPS - {FPS:.2f}\"\n",
    "        #cv2.putText(frame, str(logit), (5, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, FPS_text, (5, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e825e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470c9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310 (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
